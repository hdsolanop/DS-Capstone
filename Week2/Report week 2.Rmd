---
title: "EDA Analysis Week 2"
author: "Hernan Solano"
date: "10/10/2019"
output: html_document
---

```{r setup, include=FALSE,echo=FALSE}
require("knitr")
#Set up workin directory to easily extract english dataset
opts_knit$set(root.dir = "~/GitHub/DS-Capstone/Coursera-SwiftKey/final/en_US")
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, include = FALSE)

#Loading libraries to be used on this file
    library(tidyverse)
    library(tidytext)
    library(ggthemes)
    library(wordcloud)
```

## Introduction

In this document I will perform an Exploratory Data Analysis on the Capstone project Training Dataset provided on the Data Science Specialization. I will use tools provided by the specialization courses and also the Tidy text minig e-book that you can find here: https://www.tidytextmining.com.

```{r import_data, cache=TRUE,warning=FALSE}
#Read random lines of the file and save it into another file
        #Blog
            blogs <- read_delim("en_US.blogs.txt",delim = "\n") #read the whole blogs file
            text <- names(blogs) #Saving the name of the column because it corresponds with the first value
            names(blogs) <- "value" #Changing the name of the column to "value"
            blogs <- bind_rows(blogs,as_tibble(text)) #Completing the dataset with its first value
            blogs_rows <- floor(dim(blogs)[1]/2) #Completing the dataset with its first value
            blogs <- sample_n(blogs,blogs_rows, replace = FALSE) #Reducing the dataset to half its length choosing randomly
        #news
            news <- read_delim("en_US.news.txt", delim = "\n") #read the whole news file
            text <- names(news) #Saving the name of the column because it corresponds with the first value
            names(news) <- "value" #Changing the name of the column to "value"
            news <-  bind_rows(news,as_tibble(text)) #Completing the dataset with its first value
            news_rows <- floor(dim(news)[1]/2) #Getting the number of rows
            news <- sample_n(news,news_rows, replace = FALSE) #Reducing the dataset to half its length choosing randomly
        #twitter
            twitter <- read_delim("en_US.twitter.txt", delim = "\n") #read the whole news file
            text <- names(twitter) #Saving the name of the column because it corresponds with the first value
            names(twitter) <- "value" #Changing the name of the column to "value"
            twitter <-  bind_rows(twitter,as_tibble(text)) #Completing the dataset with its first value
            twitter_rows <- floor(dim(twitter)[1]/2) #Getting the number of rows
            twitter <- sample_n(twitter,twitter_rows,replace = FALSE) #Reducing the dataset to half its length choosing randomly
            
        #Remove unnecessary objects
            rm("blogs_rows","news_rows","twitter_rows","text")
```

## Getting to know the Dataset

Firsts things first, the dataset is divided in three files indicating the origin of the data. We have a collection of blog entries on the blogs.txt file, a collection of news extracts in the news.txt file and finally a colection of tweets on the twitter.txt file. 

What I did to get familiar with the data is to use the summary function to know the type of data that we have and the size of the data. 

Note: We are only using a randomly choosed half of the data for each file.

On the following table you can see the size and data type of half of the blogs file. The number inticates how many entries we have includes on the table.
```{r summary_blogs, include=TRUE}
#Summary of blogs
summary(blogs)
```
On the following table you can see the size and data type of half of the news file.
```{r summary_news, include = TRUE}
#Summary of news
summary(news)
```
On the following table you can see the size and data type of half of the twitter file.
```{r summary_twitter, include = TRUE}
#summary of twitter
summary(twitter)
```

### Tokenization

The next step in a simple Natural Lenguage Processing analysis is to tokenize the datasets. What you do with this is to transforme the document into a more organized format where you get a Token wich is "a meaningful unit of text, most often a word, that we are interested in using for further analysis, and tokenization is the process of splitting text into tokens" according to the Tidytext book. From there the next step is to use the a counting method to get the frequency for each token. In this case I decided to use each word as a token, first on the whole dataset and then for each source (Blogs, Twitter and news).


```{r tokenize, cache=TRUE, warning=FALSE}
#Now it is the time to tokenize the datasets and join them together
        #Blog
            blog_tk <- unnest_tokens(blogs,word,value) %>% mutate(source = "blogs")
        #news
            news_tk <- unnest_tokens(news,word,value) %>% mutate(source = "news")
        #Twitter
            twitter_tk <- unnest_tokens(twitter,word,value) %>% mutate(source = "twitter")
        #Join
            dataset <- bind_rows(blog_tk,news_tk,twitter_tk) %>%
                        mutate(word = str_extract(word, "[a-z']+")) %>% #eliminate not alphabetic characters
                        filter(!is.na(word),word !="rt") %>% #filter to keep non NA words and eliminate rt as a word
                        anti_join(stop_words) #Eliminate stopwords
```

## Exploratory Data Analysis

What I first wanted to see was the fequency of the most important words overall, that is using the whole dataset the most frequently used words are shown on the next figure.

```{r word_freq, include = TRUE}
##EXPLORATORY DATA ANALYSIS
        #Overall Word frequencies
            word_freq <- count(dataset,word, sort = TRUE)
            #Visualisation of frequencies
                word_freq %>% 
                    filter(n > 24000) %>%
                    mutate(word = reorder(word, n)) %>%
                    ggplot(aes(word, n)) + 
                    theme_economist()+
                    geom_col(fill = "#336666") +
                    xlab(NULL) +
                    coord_flip()
```

From there I wanted to understand the differences in the more common or more frequently used according to the source. We can see part of this on the next graph.

````{r word_freq_by_source, include = TRUE}
        #Word frequencies by source
            source_freq <-  count(dataset,word,source, sort = TRUE)
            #visualization of word frequencies by source
                source_freq %>%
                    filter(n > 13000) %>%
                    mutate(word = reorder(word, n)) %>%
                    ggplot(aes(word,n)) + 
                    theme_economist() +
                    geom_col(fill = "#336666") +
                    xlab(NULL) +
                    coord_flip() +
                    facet_wrap(vars(source))


```

Although this gives me some insights, I wanted to compare the frequency of the most common and less common words from each source.

````{r word_freq_by_source2, include = TRUE}
#word frequency of the most common words by source
    par(mfrow=c(1,3)) #divide the plot panell in 3 columns
    #Word frequency of most common words in Blogs
        blogs_common_freq <- count(blog_tk,word, sort = TRUE)
            #visualization of word frequencies in Blogs
                blogs_common_freq %>%
                    top_n(n = 20) %>%
                    mutate(word = reorder(word, n)) %>%
                    ggplot(aes(word,n)) + 
                    theme_economist() +
                    geom_col(fill = "#336666") +
                    xlab(NULL) +
                    coord_flip()

```


### 2-Grams analysis




### 3-Grams analysis



# Conclutions and model insights


