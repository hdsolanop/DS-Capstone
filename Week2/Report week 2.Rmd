---
title: "EDA Analysis Week 2"
author: "Hernan Solano"
date: "10/10/2019"
output: html_document
---

```{r setup, include=FALSE,echo=FALSE}
require("knitr")
#Set up workin directory to easily extract english dataset
opts_knit$set(root.dir = "~/GitHub/DS-Capstone/Coursera-SwiftKey/final/en_US")
knitr::opts_chunk$set(echo = FALSE)

#Loading libraries to be used on this file
    library(tidyverse)
    library(tidytext)
    library(ggthemes)
    library(wordcloud)
```

## Introduction

In this document I will perform an Exploratory Data Analysis on the Capstone project Training Dataset provided on the Data Science Specialization. I will use tools provided by the specialization courses and also the Tidy text minig e-book that you can find here: https://www.tidytextmining.com.

```{r import_data, cache=TRUE,warning=FALSE}
#Read random lines of the file and save it into another file
        #Blog
            blogs <- read_delim("en_US.blogs.txt",delim = "\n") #read the whole blogs file
            text <- names(blogs) #Saving the name of the column because it corresponds with the first value
            names(blogs) <- "value" #Changing the name of the column to "value"
            blogs <- bind_rows(blogs,as_tibble(text)) #Completing the dataset with its first value
            blogs_rows <- floor(dim(blogs)[1]/2) #Completing the dataset with its first value
            blogs <- sample_n(blogs,blogs_rows, replace = FALSE) #Reducing the dataset to half its length choosing randomly
        #news
            news <- read_delim("en_US.news.txt", delim = "\n") #read the whole news file
            text <- names(news) #Saving the name of the column because it corresponds with the first value
            names(news) <- "value" #Changing the name of the column to "value"
            news <-  bind_rows(news,as_tibble(text)) #Completing the dataset with its first value
            news_rows <- floor(dim(news)[1]/2) #Getting the number of rows
            news <- sample_n(news,news_rows, replace = FALSE) #Reducing the dataset to half its length choosing randomly
        #twitter
            twitter <- read_delim("en_US.twitter.txt", delim = "\n") #read the whole news file
            text <- names(twitter) #Saving the name of the column because it corresponds with the first value
            names(twitter) <- "value" #Changing the name of the column to "value"
            twitter <-  bind_rows(twitter,as_tibble(text)) #Completing the dataset with its first value
            twitter_rows <- floor(dim(twitter)[1]/2) #Getting the number of rows
            twitter <- sample_n(twitter,twitter_rows,replace = FALSE) #Reducing the dataset to half its length choosing randomly
            
        #Remove unnecessary objects
            rm("blogs_rows","news_rows","twitter_rows","text")
```

## Getting to know the Dataset

Firsts things first, the dataset is divided in three files indicating the origin of the data. We have a collection of blog entries on the blogs.txt file, a collection of news extracts in the news.txt file and finally a colection of tweets on the twitter.txt file. 

What I did to get familiar with the data is to use the summary function to know the type of data that we have and the size of the data. 

Note: We are only using a randomly choosed half of the data for each file.

On the following table you can see the size and data type of half of the blogs file. The number inticates how many entries we have includes on the table.
```{r summary_blogs}
#Summary of blogs
summary(blogs)
```
On the following table you can see the size and data type of half of the news file.
```{r summary_news}
#Summary of news
summary(news)
```
On the following table you can see the size and data type of half of the twitter file.
```{r summary_twitter}
#summary of twitter
summary(twitter)
```

#Tokenization

The next step in a simple Natural Lenguage Processing analysis is to tokenize the datasets


```{r tokenize, cache=TRUE, warning=FALSE}
#Now it is the time to tokenize the datasets and join them together
        #Blog
            blog_tk <- unnest_tokens(blogs,word,value) %>% mutate(source = "blogs")
        #news
            news_tk <- unnest_tokens(news,word,value) %>% mutate(source = "news")
        #Twitter
            twitter_tk <- unnest_tokens(twitter,word,value) %>% mutate(source = "twitter")
        #Join
            dataset <- bind_rows(blog_tk,news_tk,twitter_tk) %>%
                        mutate(word = str_extract(word, "[a-z']+")) %>% #eliminate not alphabetic characters
                        filter(!is.na(word),word !="rt") %>% #filter to keep non NA words and eliminate rt as a word
                        anti_join(stop_words) #Eliminate stopwords
```

## Exploratory Data Analysis

```{r word_freq}
##EXPLORATORY DATA ANALYSIS
        #Overall Word frequencies
            word_freq <- count(dataset,word, sort = TRUE)
            #Visualisation of frequencies
                word_freq %>% 
                    filter(n > 24000) %>%
                    mutate(word = reorder(word, n)) %>%
                    ggplot(aes(word, n)) + 
                    theme_economist()+
                    geom_col(fill = "#336666") +
                    xlab(NULL) +
                    coord_flip()
```


````{r word_freq_by_source}
        #Word frequencies by source
            source_freq <-  count(dataset,word,source, sort = TRUE)
            #visualization of word frequencies by source
                source_freq %>%
                    filter(n > 13000) %>%
                    mutate(word = reorder(word, n)) %>%
                    ggplot(aes(word,n)) + 
                    theme_economist() +
                    geom_col(fill = "#336666") +
                    xlab(NULL) +
                    coord_flip() +
                    facet_wrap(vars(source))


```
